{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv89cXUTawrS6+G9ghS/Im",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dnanper/TEST-langchain/blob/main/test03_memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xvMADjI2PFgl"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain-core>=0.3.47 langchain-openai>=0.3.9 langchain-community==0.3.16"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langsmith"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-WHNHQdAtKU",
        "outputId": "ba5541c6-2df9-4399-fc9b-45802ba0912a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langsmith in /usr/local/lib/python3.11/dist-packages (0.3.42)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith) (3.10.18)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (24.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.11.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith) (0.23.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (4.13.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1->langsmith) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langchaintest\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZHl6M_9BFiq",
        "outputId": "2b39b96d-91d2-42ba-9ec1-06a25991b6f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = os.getenv(\"OPENROUTER_API_KEY\") or getpass(\n",
        "    \"Enter OpenRouter API Key: \"\n",
        ")\n",
        "\n",
        "openrouter_model = \"deepseek/deepseek-r1:free\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0mlXldbBJCC",
        "outputId": "8c23d71d-d467-493c-903f-7f7eb1fdd6bc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter OpenRouter API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(\n",
        "    temperature=0.0,\n",
        "    api_key = os.getenv(\"OPENROUTER_API_KEY\"),\n",
        "    base_url = \"https://openrouter.ai/api/v1\",\n",
        "    model=openrouter_model\n",
        "    )"
      ],
      "metadata": {
        "id": "JbjpKsTzDcPY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Buffer Memory"
      ],
      "metadata": {
        "id": "rxLfHE3lCOfZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a built-in class: InMemoryChatMessageHistory as Message_Histoy"
      ],
      "metadata": {
        "id": "uoHLKDPvGbRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import HumanMessagePromptTemplate, SystemMessagePromptTemplate, MessagesPlaceholder, ChatPromptTemplate"
      ],
      "metadata": {
        "id": "UcJ1SKVOCOFx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a helpful assistant called Zeta.\""
      ],
      "metadata": {
        "id": "6-6SSkqOBSrx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_1 = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\")\n",
        "])"
      ],
      "metadata": {
        "id": "XN-UNtLhC7LA"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_1.input_variables"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwNwtmGHFIuP",
        "outputId": "7cb1f673-6faa-46b1-f555-6b05cc062978"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['history', 'input']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_1 = prompt_template_1 | llm"
      ],
      "metadata": {
        "id": "e3yp0JNMDKfJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory"
      ],
      "metadata": {
        "id": "JJE7ozERDSMn"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "  if session_id not in chat_map:\n",
        "    chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "  return chat_map[session_id]"
      ],
      "metadata": {
        "id": "iy8Tu4ciDmru"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "zGmTwwM0EBfv"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_1_history = RunnableWithMessageHistory(\n",
        "    pipeline_1,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ],
      "metadata": {
        "id": "jakVEhCzEDoO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_1_history.invoke(\n",
        "    {\"query\": \"hi i'm An\"},\n",
        "    config = {\"session_id\": \"01\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A41e5VyhEtnO",
        "outputId": "293331f9-1cf9-4c07-e61b-16926681e03e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi An! 👋 Nice to meet you. How can I assist you today? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 364, 'prompt_tokens': 53, 'total_tokens': 417, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747969380-uqykgKbSBAEBTNn0SkyV', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--862bf3d1-8d79-41c7-bad3-496ef141b585-0', usage_metadata={'input_tokens': 53, 'output_tokens': 364, 'total_tokens': 417, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_1_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"01\"}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58d92M_RE7Bd",
        "outputId": "6e4fbae9-7db8-4e99-8c65-c71931439b54"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is **An**! 😊 Let me know if there's anything specific you'd like help with today.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 312, 'prompt_tokens': 81, 'total_tokens': 393, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747969405-AWbs3oAIsCMm60pYSMX7', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--95091b9e-aa45-4f7b-b689-c29d75069982-0', usage_metadata={'input_tokens': 81, 'output_tokens': 312, 'total_tokens': 393, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BufferWindowMemory"
      ],
      "metadata": {
        "id": "rFA7swq3GLQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From now, all the data structure to store History Messages must be built"
      ],
      "metadata": {
        "id": "tskEUlByGme6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage"
      ],
      "metadata": {
        "id": "dhq1wP-TGHK8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BufferWindowMemory(BaseChatMessageHistory, BaseModel):\n",
        "  messages: list[BaseMessage] = Field(default_factory=list)\n",
        "  k: int = Field(default_factory=int)\n",
        "\n",
        "  def __init__(self, k:int):\n",
        "    super().__init__(k=k)\n",
        "    print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "  def add_messages(self, messages: list[BaseMessage])->None:\n",
        "    \"\"\"\n",
        "    Add messages to the history, removing any messages beyond\n",
        "    the last `k` messages.\n",
        "    \"\"\"\n",
        "    self.messages.extend(messages)\n",
        "    self.messages = self.messages[-self.k:]\n",
        "  def clear(self)->None:\n",
        "    self.messages = []"
      ],
      "metadata": {
        "id": "dbcST8WhGaHb"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_2 = {}\n",
        "def get_chat_history_2(session_id: str, k: int = 4) -> BufferWindowMemory:\n",
        "  print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "  if session_id not in chat_map_2:\n",
        "    chat_map_2[session_id] = BufferWindowMemory(k=k)\n",
        "  return chat_map_2[session_id]"
      ],
      "metadata": {
        "id": "eSQM3kRyH2fp"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec"
      ],
      "metadata": {
        "id": "8GWqBfVbIZOh"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_2_history = RunnableWithMessageHistory(\n",
        "    pipeline_1,\n",
        "    get_session_history=get_chat_history_2,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "      ConfigurableFieldSpec(\n",
        "        id=\"session_id\",\n",
        "        annotation=str,\n",
        "        name=\"Session ID\",\n",
        "        description=\"The session ID to use for the chat history\",\n",
        "        default=\"id_default\",\n",
        "      ),\n",
        "      ConfigurableFieldSpec(\n",
        "        id=\"k\",\n",
        "        annotation=int,\n",
        "        name=\"k\",\n",
        "        description=\"The number of messages to keep in the history\",\n",
        "        default=4,\n",
        "     )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "sUen4z97Ibrw"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_2_history.invoke(\n",
        "    {\"query\": \"Hi, my name is An\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSNiHeRmI-zQ",
        "outputId": "45b86ffd-d9e5-4138-f333-fc3183fe7251"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n",
            "Initializing BufferWindowMessageHistory with k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello An! It looks like your message got cut off. Could you please share your full name so I can address you properly? 😊', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 143, 'prompt_tokens': 21, 'total_tokens': 164, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747970173-7YgXkGVpDW7JMq44yovq', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--e503d647-f09d-4d13-ae09-959f55e892c6-0', usage_metadata={'input_tokens': 21, 'output_tokens': 143, 'total_tokens': 164, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_2[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map_2[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map_2[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map_2[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map_2[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map_2[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map_2[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map_2[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map_2[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map_2[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map_2[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map_2[\"id_k4\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6Q-RcxUJCtn",
        "outputId": "6afc1aa8-a481-4682-90c3-2200a5032462"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_2_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZkbE5PiSJMAA",
        "outputId": "8b0430f6-1a64-4c71-9b37-9d9aac3d0a49"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I don't have access to personal information unless you explicitly share it with me. You haven't provided your name in this conversation yet! Let me know how I can assist you. 😊\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 159, 'prompt_tokens': 64, 'total_tokens': 223, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747970256-Fd1Tbu4eqAMv5IQrX6ko', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--d6252bc5-38fb-4d42-9191-65640386a7f6-0', usage_metadata={'input_tokens': 64, 'output_tokens': 159, 'total_tokens': 223, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_2_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5AGiOvKJW9v",
        "outputId": "3a38dde7-d95c-4f42-9d7e-37f9b6e73986"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n",
            "Initializing BufferWindowMessageHistory with k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 21, 'total_tokens': 37, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747970311-VlfNnNqLRtaHaumJMcQB', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7a1613c4-24e7-4417-bcea-3c33e8a2b66f-0', usage_metadata={'input_tokens': 21, 'output_tokens': 16, 'total_tokens': 37, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_2[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map_2[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map_2[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map_2[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map_2[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map_2[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map_2[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map_2[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map_2[\"id_k14\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qgk1byIlJkWn",
        "outputId": "a982151e-bf60-484c-f2e5-0d1d5cb2c5a2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 21, 'total_tokens': 37, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747970311-VlfNnNqLRtaHaumJMcQB', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--7a1613c4-24e7-4417-bcea-3c33e8a2b66f-0', usage_metadata={'input_tokens': 21, 'output_tokens': 16, 'total_tokens': 37, 'input_token_details': {}, 'output_token_details': {}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_2_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EU6_eOo3JpSG",
        "outputId": "425d96f5-5896-48fe-bf35-1aa01e651338"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is **James**. Let me know if there's anything else you'd like to discuss! 😊\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 119, 'total_tokens': 275, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747970347-pEFJNpNYNQPSYqyll0Mu', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--47d7ee12-43a0-4365-a288-b81799821ca3-0', usage_metadata={'input_tokens': 119, 'output_tokens': 156, 'total_tokens': 275, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SumaryMemory"
      ],
      "metadata": {
        "id": "P9P0tcjnJ3h8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "class SumaryMemory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"\n",
        "        Summarize the messages and add it to the history.\n",
        "        \"\"\"\n",
        "        # self.messages.extend(messages)\n",
        "        # prompt để tạo summary mới\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # gọi llm, truyền vào summary_prompt\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=[msg.content for msg in self.messages], #summary hiện tại cho nội dung cũ\n",
        "                messages=[x.content for x in messages]  #messages hiện tại, chưa được summarize\n",
        "            )\n",
        "        )\n",
        "        # thay thế memory bằng summary mới\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ],
      "metadata": {
        "id": "9TG4qhGSJtRn"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_3 = {}\n",
        "def get_chat_history_3(session_id: str, llm: ChatOpenAI) -> SumaryMemory:\n",
        "    if session_id not in chat_map_3:\n",
        "        chat_map_3[session_id] = SumaryMemory(llm=llm)\n",
        "    return chat_map_3[session_id]"
      ],
      "metadata": {
        "id": "LlripKlRKipu"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_3_history = RunnableWithMessageHistory(\n",
        "    pipeline_1,\n",
        "    get_session_history=get_chat_history_3,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "j52yU_K0LJcV"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_3_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LV4xQ-C-LYvF",
        "outputId": "3afab0ee-a0a8-41d7-f836-ecad15eb9ec3"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! 👋 How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 107, 'prompt_tokens': 21, 'total_tokens': 128, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747971853-DqtgsLd0bq9eg3IF1THz', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--299badfa-ee8c-464f-88f0-750574afb05d-0', usage_metadata={'input_tokens': 21, 'output_tokens': 107, 'total_tokens': 128, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_3_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "chat_map_3[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcWMdxruLbXN",
        "outputId": "5a02ec1a-81cb-40f0-b717-8cb55c8b83a8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='**Updated Conversation Summary:**\\n\\n- **James** introduced himself and initiated the conversation.  \\n- **Assistant** greeted James and offered assistance.  \\n- **James** mentioned researching **types of conversational memory**, prompting the assistant to provide a detailed breakdown:  \\n  - **1. Short-Term (Buffer) Memory**: Stores recent messages (e.g., last 5-10 exchanges) for basic chatbots.  \\n  - **2. Summary Memory**: Condenses past interactions into concise summaries (e.g., tracking a user’s refund request).  \\n  - **3. Entity-Centric Memory**: Tracks specific entities (e.g., user preferences like dietary restrictions).  \\n  - **4. Long-Term (External) Memory**: Uses databases/vector stores for multi-session data (e.g., therapy bot tracking progress).  \\n  - **5. Knowledge Graph Memory**: Links entities and relationships for contextual reasoning (e.g., weather in a previously visited city).  \\n  - **6. Hybrid Memory**: Combines multiple approaches (e.g., ChatGPT’s use of context and user history).  \\n- **Challenges Highlighted**:  \\n  - Privacy and secure data storage.  \\n  - Balancing performance with memory scalability.  \\n  - Ensuring relevance of retained information.  \\n- **Assistant** concluded by offering further details or examples (e.g., tools like LangChain/Rasa) upon request.  \\n\\nThe conversation now includes a comprehensive overview of conversational memory types, challenges, and the assistant’s readiness to expand on specific topics.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_3_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ],
      "metadata": {
        "id": "6O-r6vmGN1KZ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_3[\"id_123\"].messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUUdWtB_OkSx",
        "outputId": "4798139c-36c6-4230-db99-de5dbd035bfe"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='**Updated Conversation Summary:**\\n\\n**Buffer Window Memory (Capacity: k Messages)**  \\nA fixed-size, sliding window mechanism that retains the **latest k messages**, discarding older entries.  \\n\\n**Key Features:**  \\n1. **FIFO Behavior:**  \\n   - New messages are added to the end.  \\n   - When full, the oldest message is removed.  \\n\\n2. **Use Cases:**  \\n   - Real-time analytics (e.g., tracking user actions).  \\n   - Logging systems (storing recent errors/events).  \\n   - Chat applications, network monitoring, or streaming data processing.  \\n\\n3. **Implementation:**  \\n   - **Data Structures:** Queues (e.g., Python’s `deque`), circular buffers.  \\n   - **Operations:**  \\n     - **Add:** Append messages, remove oldest if capacity is reached.  \\n     - **Retrieve:** Access all k messages in order.  \\n\\n4. **Edge Cases:**  \\n   - **k=0:** All messages are discarded.  \\n   - **k=1:** Only the latest message is stored.  \\n   - **Underflow:** Fewer than k messages result in a partially filled buffer.  \\n\\n5. **Advantages:**  \\n   - **Memory Efficiency:** Prevents unbounded growth.  \\n   - **Performance:** O(1) time complexity for insertions/deletions.  \\n\\n**Example:**  \\n- For **k=3**, input messages `A, B, C, D, E` → Buffer holds `[C, D, E]` (A and B are removed).  \\n\\n**Considerations:**  \\n- **Concurrency:** Thread-safe implementation needed for multi-threaded environments.  \\n- **Message Size:** Variable message sizes may impact memory usage despite fixed k.  \\n\\nThis structure optimizes handling of streaming data while prioritizing recent information.', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_3_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0cNJWhDQyeX",
        "outputId": "b28886cc-a039-4cca-8023-810d057744d3"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"**Answer:**  \\nThe system does not store personal information such as names. It is designed for technical use cases like real-time data processing or logging. If you need assistance with the buffer's functionality, feel free to ask!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 94, 'prompt_tokens': 435, 'total_tokens': 529, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'deepseek/deepseek-r1:free', 'system_fingerprint': None, 'id': 'gen-1747972244-DpDjCyP6wGAc7eKQa6JY', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--78ecbd6d-d8ec-4ee7-b498-db79d27e8524-0', usage_metadata={'input_tokens': 435, 'output_tokens': 94, 'total_tokens': 529, 'input_token_details': {}, 'output_token_details': {}})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SummaryBufferMemory"
      ],
      "metadata": {
        "id": "2HsNITG8P-Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummaryBufferMemory(BaseChatMessageHistory, BaseModel):\n",
        "  messages: list[BaseMessage]= Field(default_factory=list)\n",
        "  llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "  k: int = Field(default_factory=int)\n",
        "\n",
        "  def __init__(self, llm: ChatOpenAI, k: int):\n",
        "    super().__init__(llm=llm, k=k)\n",
        "\n",
        "  def add_messages(self, messages:list[BaseMessage])->None:\n",
        "    \"\"\"\n",
        "    Add messages to the history, removing any messages beyond\n",
        "    the last `k` messages and summarizing the messages that we\n",
        "    drop.\n",
        "    \"\"\"\n",
        "    existing_summary: SystemMessage | None=None\n",
        "    old_messages: list[BaseMessage] | None=None\n",
        "    # Check xem ta đã có summary chưa (lưu ở index 0 của messages history)\n",
        "    if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "      print(\">> Found existing summary\")\n",
        "      existing_summary = self.messages.pop(0) # Pop nó ra khỏi message history->lúc này chỉ còn message BaseMessage ở trong\n",
        "    # Thêm messages mới vào messages history\n",
        "    self.messages.extend(messages)\n",
        "    # Check xem số lượng messages có bị vượt quá k không\n",
        "    if len(self.messages) > self.k:\n",
        "      print(\n",
        "          f\">> Found {len(self.messages)} messages, dropping \"\n",
        "          f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "      old_messages = self.messages[:self.k] # Lấy ra những  old messages thừa, chỉ để lại k cái\n",
        "      self.messages = self.messages[-self.k:]\n",
        "    if old_messages is None:\n",
        "      print(\">> No old messages to update summary with\")\n",
        "      return  # nếu không có old message nào cả, tức không có gì để summarize\n",
        "    # tạo prompt\n",
        "    summary_prompt = ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(\n",
        "            \"Given the existing conversation summary and the new messages, \"\n",
        "            \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "            \"as much relevant information as possible.\"\n",
        "        ),\n",
        "        HumanMessagePromptTemplate.from_template(\n",
        "            \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "            \"New messages:\\n{old_messages}\"\n",
        "        )\n",
        "    ])\n",
        "    # gọi llm, với prompt có existing_summary và old_mesages: tức summary mới = existing + old_mesages\n",
        "    new_summary = self.llm.invoke(\n",
        "        summary_prompt.format_messages(\n",
        "            existing_summary=existing_summary,\n",
        "            old_messages=old_messages\n",
        "        )\n",
        "    )\n",
        "    print(f\">> New summary: {new_summary.content}\")\n",
        "    self.messages = [SystemMessage(content=new_summary.content)] + self.messages # thêm lại summary mới vào cuối messages history\n",
        "\n",
        "  def clear(self) -> None:\n",
        "    \"\"\"Clear the history.\"\"\"\n",
        "    self.messages = []"
      ],
      "metadata": {
        "id": "UhHWSRuPQBIn"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_map_4 = {}\n",
        "def get_chat_history_4(session_id: str, llm: ChatOpenAI, k: int) -> SummaryBufferMemory:\n",
        "    if session_id not in chat_map_4:\n",
        "        chat_map_4[session_id] = SummaryBufferMemory(llm=llm, k=k)\n",
        "    return chat_map_4[session_id]"
      ],
      "metadata": {
        "id": "Gto-SjYTSU1w"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_4_history = RunnableWithMessageHistory(\n",
        "    pipeline_1,\n",
        "    get_session_history=get_chat_history_4,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "uC95JRnxT1ib"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline_4_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map_4[\"id_123\"].messages\n",
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_4_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oy5yyVeUUAM6",
        "outputId": "b05166bd-e642-445f-a524-7a3d77568da7"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: **Summary of the Conversation:**\n",
            "\n",
            "James introduced himself with the message, \"Hi, my name is James.\" The AI responded warmly, saying, \"Hi James! It's nice to meet you. How can I assist you today? 😊\" This marks the start of the interaction, with the AI offering help in a friendly tone. No prior conversation history was present before this exchange.\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: **Updated Summary of the Conversation:**  \n",
            "\n",
            "James introduced himself with the message, \"Hi, my name is James.\" The AI responded warmly, offering assistance. James then mentioned he was researching conversational memory types, prompting the AI to provide a detailed breakdown of seven key architectures:  \n",
            "\n",
            "1. **Short-Term (Buffer) Memory**: Tracks recent exchanges for immediate context.  \n",
            "2. **Long-Term (Persistent) Memory**: Retains user-specific data across sessions.  \n",
            "3. **Summary Memory**: Compresses lengthy conversations into concise summaries.  \n",
            "4. **Entity Memory**: Focuses on key details like dates or locations.  \n",
            "5. **Knowledge-Grounded Memory**: Integrates external data for accuracy.  \n",
            "6. **Episodic Memory**: Stores past interactions for later recall.  \n",
            "7. **Hybrid Systems**: Combine multiple memory types for complex tasks.  \n",
            "\n",
            "The AI also highlighted tools (e.g., LangChain, Rasa, vector databases) and challenges (token limits, privacy, relevance). It concluded by offering to explore specific types or implementation examples further. The conversation transitioned from introductions to an in-depth technical discussion, emphasizing practical applications and frameworks.\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: **Updated Summary of the Conversation:**\n",
            "\n",
            "The conversation explores conversational memory types, initially outlining seven key architectures:  \n",
            "1. **Short-Term (Buffer) Memory** (immediate context),  \n",
            "2. **Long-Term (Persistent) Memory** (user-specific data across sessions),  \n",
            "3. **Summary Memory** (condensed conversation summaries),  \n",
            "4. **Entity Memory** (tracking key details like dates/locations),  \n",
            "5. **Knowledge-Grounded Memory** (integrating external data),  \n",
            "6. **Episodic Memory** (recalling past interactions),  \n",
            "7. **Hybrid Systems** (combining multiple memory types).  \n",
            "\n",
            "The discussion then focuses on two specific implementations:  \n",
            "\n",
            "### **ConversationBufferMemory**  \n",
            "- **Function**: Stores the **entire conversation history** in a buffer (e.g., LangChain's `ConversationBufferMemory`).  \n",
            "- **Use Case**: Ideal for short, context-heavy interactions requiring full history retention.  \n",
            "- **Example**: Retains all messages for tasks like summarizing chats or multi-step processes.  \n",
            "- **Limitations**: Risks exceeding token limits in long conversations.  \n",
            "\n",
            "### **ConversationBufferWindowMemory**  \n",
            "- **Function**: Retains a **fixed window of recent interactions** (e.g., last *k* messages via LangChain's `ConversationBufferWindowMemory`).  \n",
            "- **Use Case**: Balances context retention with token efficiency by discarding older messages.  \n",
            "- **Example**: Keeps the last 2–5 exchanges for quick Q&A bots.  \n",
            "- **Limitations**: Loses older context if not managed.  \n",
            "\n",
            "**Key Differences**:  \n",
            "| Feature                | BufferMemory                          | BufferWindowMemory                     |  \n",
            "|------------------------|---------------------------------------|----------------------------------------|  \n",
            "| **Retention**          | Entire history                        | Last *k* interactions                  |  \n",
            "| **Token Usage**         | Grows with conversation length        | Fixed (controlled by *k*)              |  \n",
            "| **Ideal For**           | Short, context-heavy tasks            | Conversations with token constraints   |  \n",
            "\n",
            "**Tools & Challenges**:  \n",
            "- **Tools**: LangChain classes (`ConversationBufferMemory`, `ConversationBufferWindowMemory`), Rasa, vector databases.  \n",
            "- **Challenges**: Token limits (e.g., GPT-4's context window), privacy, relevance of retained data.  \n",
            "\n",
            "The conversation emphasizes practical implementation, including code examples and hybrid approaches (e.g., combining buffer windowing with summary memory). The focus is on optimizing memory systems for real-world applications like chatbots and virtual assistants.\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping oldest 2 messages.\n",
            ">> New summary: **Updated Summary of the Conversation:**  \n",
            "\n",
            "The discussion focuses on **ConversationBufferMemory** and **ConversationBufferWindowMemory**, two key memory architectures in frameworks like LangChain, with expanded implementation details and use-case guidance:  \n",
            "\n",
            "---\n",
            "\n",
            "### **Key Memory Types**  \n",
            "1. **ConversationBufferMemory**  \n",
            "   - **Function**: Stores the **entire conversation history** as a raw text buffer.  \n",
            "   - **Use Cases**:  \n",
            "     - Short, context-heavy tasks requiring full history (e.g., chat summarization, multi-step workflows).  \n",
            "     - Ideal when token limits are not a concern (e.g., GPT-4’s 128k window for brief interactions).  \n",
            "   - **Pros**: Simple implementation, complete context retention.  \n",
            "   - **Cons**: Risk of exceeding token limits in long conversations; may retain irrelevant details.  \n",
            "   - **Code Example (LangChain)**:  \n",
            "     ```python  \n",
            "     memory = ConversationBufferMemory()  \n",
            "     memory.save_context({\"input\": \"Hi!\"}, {\"output\": \"Hello!\"})  # Appends all interactions  \n",
            "     ```  \n",
            "\n",
            "2. **ConversationBufferWindowMemory**  \n",
            "   - **Function**: Retains a **fixed window of the last `k` interactions** (e.g., last 2–5 messages).  \n",
            "   - **Use Cases**:  \n",
            "     - Token-constrained environments (e.g., GPT-3.5’s 4k window).  \n",
            "     - Quick Q&A bots where only recent context matters.  \n",
            "   - **Key Parameter**: `k` (number of retained interactions), requiring tuning for balance.  \n",
            "   - **Pros**: Prevents token overflow; focuses on relevant recent exchanges.  \n",
            "   - **Cons**: Loses older context that might still be critical.  \n",
            "   - **Code Example (LangChain)**:  \n",
            "     ```python  \n",
            "     memory = ConversationBufferWindowMemory(k=2)  # Keeps last 2 interactions  \n",
            "     memory.save_context({\"input\": \"Weather?\"}, {\"output\": \"Sunny!\"})  \n",
            "     ```  \n",
            "\n",
            "---\n",
            "\n",
            "### **Comparison Table**  \n",
            "| Feature                | BufferMemory                          | BufferWindowMemory                     |  \n",
            "|------------------------|---------------------------------------|----------------------------------------|  \n",
            "| **Retention**          | Entire history                        | Last `k` interactions                  |  \n",
            "| **Token Usage**        | Grows with conversation length        | Fixed (controlled by `k`)              |  \n",
            "| **Use Case**           | Short, context-heavy tasks            | Token-constrained/long conversations   |  \n",
            "| **Implementation**     | Simple (no tuning)                    | Requires tuning `k` for optimization   |  \n",
            "\n",
            "---\n",
            "\n",
            "### **Hybrid Approaches**  \n",
            "- Combine **BufferWindowMemory** (recent context) with **SummaryMemory** (compressed older interactions) to balance token efficiency and critical historical data.  \n",
            "- Example: Use a sliding window for immediate context while summarizing earlier parts of long conversations.  \n",
            "\n",
            "---\n",
            "\n",
            "### **Tools & Challenges**  \n",
            "- **Tools**: LangChain classes, Rasa, vector databases.  \n",
            "- **Challenges**:  \n",
            "  - Token limits (model-specific, e.g., GPT-3.5 vs. GPT-4).  \n",
            "  - Privacy concerns with persistent storage.  \n",
            "  - Relevance vs. efficiency trade-offs.  \n",
            "\n",
            "The conversation emphasizes practical implementation strategies, including code examples and hybrid systems, to optimize memory management for applications like chatbots and virtual assistants.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0V7U4U1aUC0q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}